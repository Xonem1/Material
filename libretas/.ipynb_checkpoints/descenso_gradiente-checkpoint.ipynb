{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/letragrama-rgb-150.jpg\" width=\"380\" align=\"left\"><img src=\"imagenes/logoLCCazul.jpg\" width=\"170\" align=\"right\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Aprendizaje en redes neuronales: Descenso de gradiente y sus modificaciones*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 18 de septiembre de 2017.\n",
    "\n",
    "El algoritmo más sencillo de aprendizaje para redes neuronales el el descenso de gradiente, tal y como lo vimos en libretas pasadas. Sin embargo vimos que es un poco complicado poder establecer muchos parámetros del algoritmo de aprendizaje, además que el método tiende a quedarse en el primer mínimo local que encuentra, por esta razón, amos a ver varias de las modificaciones más comunes al algoritmo de descenso de gradiente, tal y como lo vimos anteriormente.\n",
    "\n",
    "Vamos a asumir que todos han realizado la libreta anterior donde desarrolló, validó y probó el método de *backpropagation\" para una red neuronal (a cual ya habíamos estavlecido antes, y que todo mundo tiene el archivo `feedforward.py` con las funciones básicas y tiene programado el *b-prop* así como el método clásico de descenso de gradiente.\n",
    "\n",
    "Si no es así, les recomiendo hacer primero las otras dos libretas y regresar a esta una vez acabadas las anteriores.\n",
    "\n",
    "#### Ejercicio: Completa las funciones con lo que realizaste en la libeta anterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from feedforward import *\n",
    "\n",
    "def r_fun(matriz):\n",
    "    return matriz[:, 1:]\n",
    "\n",
    "def backpropagation(Y, A, rn):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de los pesos de una red neuronal\n",
    "    \n",
    "    Parametros\n",
    "    -----------\n",
    "    Y: ndarray de shape (N, k) donde N es el número de ejemplos y k el número de salidas\n",
    "    \n",
    "    A: Una lista de matrices de activaciones por capas, obtenidas por la función `feedforward`,\n",
    "       en donde A[l] es un ndarray de shape (nl, N), donde N es el número de ejemplos evaluados \n",
    "       y nl es el número de neuronas de la capa l de rn.\n",
    "\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función \n",
    "        `inicializa_red_neuronal`\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Una lista de gradientes [None, gW1, gW2, ..., gWL], donde cada gWl es un ndarray tal que \n",
    "    rn['W'][l].shape == gWl.shape\n",
    "             \n",
    "    \"\"\"    \n",
    "    N = Y.shape[0]\n",
    "    gradientes = [None] + [np.zeros_like(Wl) for Wl in rn['W'][1:]]\n",
    "    delta = Y.T - A[-1]\n",
    "    \n",
    "    #Calcula el gradiente de los pesos de la última capa\n",
    "    gradientes[-1] = -delta.dot(extendida(A[-2]).T) / N\n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for l in range(rn['capas'] - 2, 0, -1):         \n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "        \n",
    "        # Calcula la delta para la capa anterior.  \n",
    "        \n",
    "        # Calcula el gradiente para los pesos de la capa anterior.        \n",
    "\n",
    "                \n",
    "    return gradientes\n",
    "\n",
    "\n",
    "def desc_grad_simple(rn, X, Y, alfa=0.2, max_epochs=1000, normaliza=False):\n",
    "    \"\"\"\n",
    "    Entrena una red neuronal utilizando el método de descenso de gradiente simple\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    Y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    alfa: La tasa de aprendizaje, un número positivo típicamente menor que 1\n",
    "    max_epochs: Número de epocas o iteraciones del algoritmo. Un entero positivo.\n",
    "    normaliza: Un booleano para usar X y Y para normalizar o no los datos antes de entrar a la rn.\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    None, la función utiliza el hecho que rn es un objeto mutabe y modifica rn['W'] directamente.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Encuentra medias y desviaciones si aplica\n",
    "    if normaliza:\n",
    "        rn['medias'], rn['std'] = obtiene_medias_desviaciones(X)\n",
    "        \n",
    "    # Aprendizaje\n",
    "    for _ in range(max_epochs):\n",
    "        # -------------------------------------------\n",
    "        # Insertar código aquí\n",
    "        # -------------------------------------------\n",
    "\n",
    "        # Calcula las activaciones\n",
    "        \n",
    "        # Calcula lss gradientes\n",
    "\n",
    "        # Actualiza los pesos\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
