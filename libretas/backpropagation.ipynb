{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/letragrama-rgb-150.jpg\" width=\"380\" align=\"left\"><img src=\"imagenes/logoLCCazul.jpg\" width=\"170\" align=\"right\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Aprendizaje en redes neuronales: El algoritmo de *backpropagation*\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 18 de septiembre de 2017.\n",
    "\n",
    "En esta libreta vamos a practicar y revisar el principal algoritmo de aprendizaje de los pesos de una  red neuronal hacia adelante. Todos los algoritmos más sofisticados son versiones y modificaciones al algoritmo de *backpropagation* o *B-prop*. Para aplicaciones reales vamos autilizar herramientas poderosas como [Tensorflow](https://www.tensorflow.org) o [Theano](http://www.deeplearning.net/software/theano/), pero es importante conocer y saber desarrollar el algoritmo básico para entender como funciona el aprendizaje en general en redes neuronales.\n",
    "\n",
    "Vamos a asumir que todos han realizado la libreta anterior donde desarrollaron una estructura básica de una red neuronal, así como el algoritmo de reconocimiento, o *feedforward*. Para una explicacion de la estructura, es necesario que se remitan a la libreta anterior. Sin embarog resumirenos los pasos principales:\n",
    "\n",
    "\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir, así como las funciones desarrolladas en otras libretas para el calculo en feedforward de redes neuronales artificiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from feedforward import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones ya programadas las dejamos en un archivo adjunto que se llama `feedforward.py` el cual contiene las operaciones que se desarrollaron anteriormente y que agregarlas en esta libreta metería más ruido de lo que ayudaría a la comprensión. El archivo se encuentra adjunto y eres libre de consultarlo y revisarlo. En este caso nos comportamos muy poco formales y cargamos todas las funciones en el espacio de trabajo principal. Esto es porque vamos a hacer de cuenta que las acabamos de programar.\n",
    "\n",
    "En las libretas de *Jupyter*, cuando quieres consultar la documentación de una función utilizas la combinación de teclas *shift*-*tab*. Si lo haces una vez aparece únicamente el docstring en una ventana emergente. Si utilizas *shift*-*tab* dos veces, aparece la documentación extendida (si hay), y con 3 veces la documentación completa pero en ventana emergente. Con 4 veces la documentación completa aparece en un *frame* abajo y se mantiene mientras tecleas, hasta que se cierre. Este ultimo es especialmente útil para revisar documentación de una función mientras decide uno como utilizarla.\n",
    "\n",
    "#### Ejercicio 1: Revisa la documentación de las funciones que se listan a continuación, ya que las vamos a estar utilizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function feedforward.perdida_red>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Revisa la documentación de cada uno de las soguientes funciones\n",
    "\n",
    "inicializa_red_neuronal\n",
    "logistica\n",
    "extendida\n",
    "feedforward\n",
    "perdida\n",
    "perdida_red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Como puedes ver las funciones anteriores (que puedes consultar en el archivo `feedforard.py`), nos permiten definir una red neuronal, estimar la salida para un conjunto de datos, así como estimar la función de pérdda con el mismo, u otro conjunto de datos.\n",
    "\n",
    "## El algortmo de *backpropagation*\n",
    "\n",
    "La derivada parcial de la función de pérdida respecto a cada uno de los pesos (por lo tanto el gradiente de la función de pérdida respecto al vector de todos los pesos) se obtiene mediante el algoritmo de backpropagarion. Una vez que se conoce esto, entonces es posible utilizar el resultado para optimizar los pesos mediente el método de descenso de gradiente o una variente similar.\n",
    "\n",
    "El algoritmo de *backpropagation* (de hecho en los últimos años se conoce como *b-prop*, y solo los rucos decimos *backpropagation*) es el siguiente:\n",
    "\n",
    "1. Calcular el vector $\\delta^{(L)}$ para todos los datos a partir de $A^{L}$, $Y$ y el tipo de salida. Si utilizamos la función de pérdida correcta para cada tipo de salida, entonces esto se calcula en forma muy sencilla, tal como vimos en clases de la forma $$\\delta^{(L)} = Y^T - A^{(L)}.$$\n",
    "\n",
    "2. Para $l$ de $L-1$ hasta 1 con decrementos de $-1$:\n",
    "    1. Calcular $\\delta^{(l)}$ a partir de $\\delta^{(l+1)}$, $W^{(l+1)}$ y $A^{(l)}$ como\n",
    "       $$\n",
    "       \\delta^{(l)} = A^{(l)} \\star (\\vec{1} - A^{(l)}) \\star (r(W^{(l+1)})^T \\delta^{(l+1)}), \n",
    "       $$\n",
    "       donde $\\star$ es la multiplicación elemento a elemento de dos matrices, $\\vec(1)$ es una matriz con 1 en todos sus elementos y $r(\\cdot)$ es una función que elimina la primer columna de una matriz.\n",
    "    2. Calcular la derivada en cada uno de los pesos como\n",
    "       $$\n",
    "       \\frac{\\partial J(W)}{w_{ij}^{(l)}} = \\frac{1}{card(CA)}\\sum_{\\forall (x,y) \\in CA} a_j^{(l-1)} \\delta_i^{(l)},\n",
    "       $$\n",
    "       para lo cual vimos como realizarlo en forma matricial y eficiente en clases.\n",
    "       \n",
    "El desarrollo del algoritmo y los pasos en forma matricial se vieron con detalle en clase, por lo que aqui solo se da un pequeñño bosquejo esperando que se programe correctamente.\n",
    "\n",
    "#### Ejercicio 1: Completa el código de la función de backpropagation (20 puntos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La función `r`, cambiada como `r_fun` para evitar confuciones \n",
    "def r_fun(matriz):\n",
    "    return matriz[:, 1:]\n",
    "\n",
    "def backpropagation(Y, A, rn):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de los pesos de una red neuronal\n",
    "    \n",
    "    Parametros\n",
    "    -----------\n",
    "    Y: ndarray de shape (N, k) donde N es el número de ejemplos y k el número de salidas\n",
    "    \n",
    "    A: Una lista de matrices de activaciones por capas, obtenidas por la función `feedforward`,\n",
    "       en donde A[l] es un ndarray de shape (nl, N), donde N es el número de ejemplos evaluados \n",
    "       y nl es el número de neuronas de la capa l de rn.\n",
    "\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función \n",
    "        `inicializa_red_neuronal`\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Una lista de gradientes [None, gW1, gW2, ..., gWL], donde cada gWl es un ndarray tal que \n",
    "    rn['W'][l].shape == gWl.shape\n",
    "             \n",
    "    \"\"\"    \n",
    "    # Numero de ejemplos\n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # Inicializa la lista de gradientes en 0\n",
    "    gradientes = [None] + [np.zeros_like(Wl) for Wl in rn['W'][1:]]\n",
    "\n",
    "    # Calcula Delta para la capa de salida\n",
    "    delta = Y.T - A[-1]\n",
    "    \n",
    "    #Calcula el gradiente de los pesos de la última capa\n",
    "    gradientes[-1] = -delta.dot(extendida(A[-2]).T) / N\n",
    "    \n",
    "    # Despues vamos a hacer lo propio por cada capa hasta antes de la última\n",
    "    for l in range(rn['capas'] - 2, 0, -1):         \n",
    "        \n",
    "        # Calcula la delta para la capa anterior.        \n",
    "        # ----------Agregar código aqui -----------------\n",
    "        delta = A[l] * (1 - A[l]) * (r_fun(rn['W'][l + 1]).T.dot(delta))\n",
    "        \n",
    "        \n",
    "        # Calcula el gradiente para los pesos de la capa anterior.        \n",
    "        # ----------Agregar código aqui -----------------\n",
    "        gradientes[l] = -delta.dot(extendida(A[l-1]).T) / N\n",
    "                \n",
    "    return gradientes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar y corregir código de aprendizaje máquina\n",
    "\n",
    "El problema con este tipo de algoritmos es que, a la hora de codificarlos, es muy típico que se tengan errores, tanto de concepto como de codificación. Sin embargo, como estos algortimos se utilizan para aprender, y al utilizar un conjunto inicial de pesos aleatorio diferente, los resultados no son verificables. Es muy común tener un error muy tonto y pasar muchas horas (o días) intentando corregirlo.\n",
    "\n",
    "Por esta razón, siempre hay que hacer métodos que nos permitan chacar que el algortimo parece funcionar correctamente. Para eso, vamos a programar una forma alternativa de calcular una aproximación del gradiente en forma numérica. Este algortimo de fuerza bruta es altamente ineficiente y no puede ser utilizada dentro de un método de optimización pero nos sirve para revisar si existen errores que podríamos haber ingresado en nuestro algoritmo.\n",
    "\n",
    "El método se basa en el calculo numérico de una derivada parcial como:\n",
    "\n",
    "$$\n",
    "  \\left.\\frac{\\partial f(x)}{\\partial x}\\right|_{x = x_0} \\approx \\frac{f(x_0 + \\epsilon) - f(x_0 - \\epsilon)}{2 \\epsilon}.  \n",
    "$$\n",
    "\n",
    "Entonces, si queremos estimar el gradiente de la función de pérdida respecto a los pesos, hay que calcular esta razón por cada uno de los pesos, por cada una de las capas. Esto no es nada eficiente y mucho menos elegante (como el *b-prop*) pero es un método de validación.\n",
    "\n",
    "#### Ejercicio: Completa el código de la función de gradiente numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradiente_numerico(X, Y, rn, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente numérico para efectos de prueba del algoritmo de backpropagation.\n",
    "    Este algortimo se realiza expresamente de manera ineficiente pero clara, ya que su\n",
    "    funcionamiento debe de ser lo más claro posible.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    X: ndarray de shape (T, n) donde T es el número de ejemplos y n el número de atributos\n",
    "    Y: ndarray de shape (T, k) donde k es el número de salidas\n",
    "    rn: Estructura de datos de una red neuronal inicializada con la función `inicializa_red_neuronal``\n",
    "    epsilon: Un número flotante positivo, típicamente muy pequeño\n",
    " \n",
    "    Devuelve\n",
    "    --------\n",
    "    Una lista de gradientes de red_neuronal['W'] con la misma estructura y dimensión\n",
    "    \n",
    "    \"\"\"\n",
    "    # inicializa los gradientes a cero\n",
    "    gradientes = [None] + [np.ones_like(Wl) for Wl in rn['W'][1:]]\n",
    "    \n",
    "    for l in range(1,rn['capas']):                #  Por cada capa l\n",
    "        for i in range(rn['W'][l].shape[0]):          #  Por cada renglon i\n",
    "            for j in range(rn['W'][l].shape[1]):      #  Por cada columna j\n",
    "                # -------------------------------------------\n",
    "                # Insertar código aquí\n",
    "                # -------------------------------------------\n",
    "                rn['W'][l][i, j] -= epsilon\n",
    "                f_1 = perdida_red(Y, X, rn)\n",
    "                rn['W'][l][i, j] += 2*epsilon\n",
    "                f_2 = perdida_red(Y, X, rn)\n",
    "                rn['W'][l][i, j] -= epsilon\n",
    "                gradientes[l][i, j] = (f_2 - f_1) / (2 * epsilon)                \n",
    "                # --------------------------------------------\n",
    "    return gradientes           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer una función de prueba utilizando un conjunto de datos reales (o un subconjunto de estos), y lo vamos a hacer para muchas posibles reinicializaciones de pesos. Este código va a servir para corregir ambos algoritmos de calculo de gradiente.  Para esto vamos a utilizar una base de datos ya conocida, la de dígitos, utilizada en la libreta de regresión softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Datos a utilizar para la prueba\n",
    "# para no tener que estarlos cargando de nuevo cada vez\n",
    "\n",
    "# Vamos a utilizar un subconkunto de datos y de atributos\n",
    "# para que pueda funcionar el gradiente numérico en un tiempo \n",
    "# aceptable\n",
    "\n",
    "\n",
    "data = np.load(\"datos/digitos.npz\")\n",
    "x_prueba = data['X_entrena'][:100,:10]  # Solo 100 datos y 10 parámetros\n",
    "y_prueba = data['T_entrena'][:100,:]    # Todas las clases de los primeros 100 datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Favor de no seguir más allá en la programación de la red neuronal hasta estar seguro que esto funcione correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: lineal, Neuronas: [10, 10, 10]\n",
      "Máxima diferencia entre métodos de gradiente: 8.830145559191038e-09\n",
      "Tipo: logistica, Neuronas: [10, 10, 10]\n",
      "Máxima diferencia entre métodos de gradiente: 1.443205288609306e-08\n",
      "Tipo: softmax, Neuronas: [10, 10, 10]\n",
      "Máxima diferencia entre métodos de gradiente: 0.09000000000000011\n",
      "Gradiente: \n",
      "[-0.09       -0.04310492 -0.04396376 -0.04817452 -0.04632102 -0.04204483\n",
      " -0.05229717 -0.03850418 -0.04073343 -0.0426418  -0.03883266]\n",
      "Gradiente Numérico: \n",
      "[  0.00000000e+00  -2.64288323e-04  -1.17394739e-04   1.79676216e-05\n",
      "   1.13409682e-04  -1.64521117e-04   5.96803722e-05  -3.22597286e-04\n",
      "  -3.58586429e-04  -3.20916756e-04  -1.85745588e-04]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0ba3a0b0a457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Paso la prueba\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprueba_gradiente\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_prueba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prueba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0ba3a0b0a457>\u001b[0m in \u001b[0;36mprueba_gradiente\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gradiente Numérico: \\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradientes_numericos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiferencias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Paso la prueba\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prueba_gradiente(X, Y):\n",
    "    \"\"\"\n",
    "    Unidad de prueba de backpropagation\n",
    "    \n",
    "    \"\"\"\n",
    "    n0 = X.shape[1]\n",
    "    nL = Y.shape[1]\n",
    "    \n",
    "    for ocultas in [1, 2, 3]: \n",
    "        for neuronas in [1, 2]:\n",
    "            rn = inicializa_red_neuronal(ocultas + 2, \n",
    "                                         [n0] + (ocultas * [neuronas * n0]) + [nL],\n",
    "                                         'lineal')\n",
    "\n",
    "            for tipo in ['lineal', 'logistica', 'softmax']:\n",
    "                rn['tipo'] = tipo\n",
    "                A = feedforward(X, rn)\n",
    "                gradientes = backpropagation(Y, A, rn)\n",
    "                gradientes_numericos = gradiente_numerico(X, Y, rn, 1e-3)\n",
    "                diferencias = [np.abs(g - gn).max() \n",
    "                               for (g, gn) in zip(gradientes[1:], \n",
    "                                                  gradientes_numericos[1:])]\n",
    "                print(\"Tipo: {}, Neuronas: {}\".format(rn['tipo'], rn['nxc']))\n",
    "                print(\"Máxima diferencia entre métodos de gradiente: {}\".format(max(diferencias)))\n",
    "                if max(diferencias) > 1e-3:\n",
    "                    print('Gradiente: \\n{}'.format(gradientes[-1][0,:]))\n",
    "                    print('Gradiente Numérico: \\n{}'.format(gradientes_numericos[-1][0,:]))\n",
    "\n",
    "                assert max(diferencias) < 1e-3\n",
    "    return \"Paso la prueba\"\n",
    "\n",
    "print(prueba_gradiente(x_prueba, y_prueba))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
